{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c036f4-1fb9-4a3e-8ddc-dd95a7af835e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "from copy import deepcopy\n",
    "from torchvision.datasets import MNIST   #Torchvision used only to import the MNIST dataset!\n",
    "from torchvision.transforms import ToTensor     #it plays no role in the model calculations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f9524f-3cb7-4e66-890e-186874f5c6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):    #sigmoid\n",
    "    return 1 / (1 + np.exp(-x / 10))\n",
    "\n",
    "def sig(x):    #numpy vectorized sigmoid\n",
    "    y = np.vectorize(sigmoid)\n",
    "    return y(x)\n",
    "\n",
    "def sig_diff(x):   #sigmoid derivative\n",
    "    return sig(x) * (1 - sig(x)) \n",
    "\n",
    "def diff_ReLU(x):   #ReLU derivative\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "def dReLU(x):       #vectorized ReLU derivative\n",
    "    y = np.vectorize(diff_ReLU)\n",
    "    return y(x)\n",
    "\n",
    "def ReLU(x):        #ReLU\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249dd06f-75a8-4cd7-af00-5df46761dacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Neural():  #class for the neural network\n",
    "    \n",
    "    def __init__(self, LR): #sizes are static but the code is flexible so they can easily be changed\n",
    "        self.LR = LR        #or turned into __init__ parameters \n",
    "        self.weights1 = np.random.uniform(-0.5, 0.5, size = (2048, 28*28))\n",
    "        self.weights2 = np.random.uniform(-0.5, 0.5, size = (10, 2048))\n",
    "        self.bias1 = np.zeros(2048)\n",
    "        self.bias2 = np.zeros(10)\n",
    "        self.deltaw1, self.deltaw2, self.deltab1, self.deltab2 = 0, 0, 0, 0\n",
    "        #LR - Learning rate, deltas store gradients for a batch\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.logits = [x]\n",
    "        self.activations = []\n",
    "        x = np.matmul(self.weights1, x) + self.bias1\n",
    "        self.logits.append(x)\n",
    "        x = ReLU(x)\n",
    "        self.activations.append(x)\n",
    "        x = np.matmul(self.weights2, x) + self.bias2\n",
    "        self.logits.append(x)\n",
    "        x = sig(x)\n",
    "        self.activations.append(x)\n",
    "        #forward pass, saves logits under self.logits and\n",
    "        #activations under self.activations\n",
    "            \n",
    "    def loss_d(self, target):\n",
    "        if type(target) != {list, np.array}:\n",
    "            Y = np.zeros(len(self.bias2))\n",
    "            Y[target] = 1\n",
    "            target = Y\n",
    "        length = len(target)\n",
    "        x = -2 * (self.activations[1] - target) / 10\n",
    "        return x\n",
    "        #returns gradient of the loss function w.r.t. activations in last layer\n",
    "    \n",
    "    def grad2(self, target):\n",
    "        x = []\n",
    "        for j in range(len(self.weights2[1])):\n",
    "            row = np.sum(self.dzda * self.weights2[:, j]) * dReLU(self.logits[1][j])\n",
    "            x.append(row)\n",
    "        x = np.array(x)\n",
    "        return x\n",
    "        #returns gradient of loss fn w.r.t activations in layer 1\n",
    "        \n",
    "    def backward(self, target):\n",
    "        self.dzda =  self.loss_d(target) * sig_diff(self.logits[2])\n",
    "        #dzda stores gradient of activations in last layer (activations2) w.r.t.\n",
    "        #logits in each layer, updating along, here layer 2 (last)\n",
    "        \n",
    "        self.dw2 = np.matmul(np.expand_dims(self.dzda, 1), np.expand_dims(self.activations[0].T, 0))\n",
    "        #gradient of loss w.r.t. weigths 2\n",
    "        self.db2 = deepcopy(self.dzda)\n",
    "        #gradient of loss w.r.t. bias 2\n",
    "        \n",
    "        self.dzda = self.grad2(target)\n",
    "        #here dzda shows grad of activations2 w.r.t. logits 1 \n",
    "        \n",
    "        self.db1 = deepcopy(self.dzda)\n",
    "        #gradient of loss w.r.t. bias 1\n",
    "        self.dw1 = np.dot( np.expand_dims(self.dzda, 1), np.expand_dims(self.logits[0].T, 0) )\n",
    "        #gradient of  loss w.r.t. weigths 1\n",
    "        \n",
    "        self.deltaw1 += self.dw1 * self.LR\n",
    "        self.deltaw2 += self.dw2 * self.LR\n",
    "        self.deltab1 += self.db1 * self.LR\n",
    "        self.deltab2 += self.db2 * self.LR\n",
    "        #storing gradients of params in a batch to apply an average afterwards\n",
    "        \n",
    "    def train(self, data):  #training the model, data must be a list, np.array\n",
    "        self.deltaw1, self.deltaw2, self.deltab1, self.deltab2, = 0, 0, 0, 0\n",
    "        #reseting the gradients \n",
    "        length = len(data)\n",
    "        for item in data:\n",
    "            (X, Y) = item      #X - logits 0, Y - label\n",
    "            X = X.numpy().flatten()  #Flattening the image from a matrix into a 1d array\n",
    "            self.forward(X)    #forward pass\n",
    "            self.backward(Y)   #backward pass, calculating gradients\n",
    "        self.weights1 += self.deltaw1 / length\n",
    "        self.weights2 += self.deltaw2 / length\n",
    "        self.bias1 += self.deltab1 / length\n",
    "        self.bias2 += self.deltab2 / length\n",
    "        #adding the average of gradients in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af39aab-6d13-4ecc-8bfa-dfd61369cf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = Neural(LR = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d6f41f-6c67-4e96-9a5c-d9bf097d7ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = MNIST(root = 'MNIST',\n",
    "                   train = True,\n",
    "                   download = False,   #set to True to download\n",
    "                   transform = ToTensor(),\n",
    "                   target_transform = None)\n",
    "test_data = MNIST(root = 'MNIST',\n",
    "                  train =False,\n",
    "                  download = False,   #set to True to download\n",
    "                  transform = ToTensor(),\n",
    "                  target_transform = None)\n",
    "                  #setting up train and test data\n",
    "train_data_1 = list(train_data)\n",
    "test_data_1 = list(test_data)\n",
    "train_data_whole = deepcopy(train_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fef1e35-b464-429c-8a6d-388b241f8773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "measurements = [[],[],[],[]]  #estabilishing a vector of measurements to be able to plot/examine the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb83ad7-e1ae-41e4-a93e-60ac1e6aac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LR = 0.01\n",
    "i = 0  #batch number\n",
    "Batch_Size = 16\n",
    "while len(train_data_1) > 0:   #one pass over the training dataset\n",
    "\n",
    "    if i % 5 == 0:   #evaluate the metrics every 5th batch\n",
    "        loss_total, loss_train, acc_total, acc_train = 0, 0, 0, 0 #init measured values\n",
    "        for test in test_data_1:\n",
    "            X, Y = test\n",
    "            X = X.numpy().flatten()\n",
    "            y = np.zeros(10)\n",
    "            y[Y] = 1\n",
    "            nn.forward(X)\n",
    "            loss = nn.activations[1] - y\n",
    "            loss = np.sum([x ** 2 for x in loss]) / 10  #computing loss function\n",
    "            loss_total += loss\n",
    "            if np.argmax(nn.activations[1]) == Y:  #checking if prediction matches label\n",
    "                acc_total += 1\n",
    "\n",
    "        for test in train_data_whole:\n",
    "            X, Y = test\n",
    "            X = X.numpy().flatten()\n",
    "            y = np.zeros(10)\n",
    "            y[Y] = 1\n",
    "            nn.forward(X)\n",
    "            loss = nn.activations[1] - y\n",
    "            loss = np.sum([x ** 2 for x in loss]) / 10\n",
    "            loss_train += loss\n",
    "            if np.argmax(nn.activations[1]) == Y:\n",
    "                acc_train += 1\n",
    "        print(f'{i} | Test_set: loss : {round(loss_total / 10000, 4)}, \\\n",
    "        acc : {round(acc_total / 100, 3)}% \\\n",
    "        | Train_set: loss : {round(loss_train / 60000, 4)}, \\\n",
    "        acc : {round(acc_train / 600, 3)}%')\n",
    "        measurements[0].append(loss_total / 10000)\n",
    "        measurements[1].append(acc_total / 100)\n",
    "        measurements[2].append(loss_train / 60000)\n",
    "        measurements[3].append(acc_train / 600)\n",
    "\n",
    "    nn.train(train_data_1[:Batch_Size:])\n",
    "    train_data_1 = train_data_1[Batch_Size::]\n",
    "    #moving onto the next batch\n",
    "\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
